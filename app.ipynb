{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import openai\n",
    "import re \n",
    "import os \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the 'punkt' tokenizer from NLTK\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../api.key', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    # Remove any extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\Dropbox\\Desktop\\MAchineLearning\\openai\\embedding_littleprince\\\\thelittleprince.pdf\" \n",
    "start_page = 1\n",
    "num_pages = 0\n",
    "\n",
    "sentences_list = []\n",
    "\n",
    "reader = PyPDF2.PdfReader(path)\n",
    "\n",
    "num_pages = len(reader.pages)\n",
    "print(num_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"thelittleprince.txt\", \"w\") as f:\n",
    "    for i in range(start_page-1, num_pages):\n",
    "        page = reader.pages[i]\n",
    "        text = page.extract_text() \n",
    "        text = preprocess(text) \n",
    "        f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open(\"thelittleprince.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'sentences' folder if it doesn't exist\n",
    "if not os.path.exists(\"sentences\"):\n",
    "    os.makedirs(\"sentences\")\n",
    "\n",
    "# Save x sentences in a separate file\n",
    "num_sentences_per_file = 15\n",
    "\n",
    "for i in range(0, len(sentences), num_sentences_per_file):\n",
    "    filename = f\"sentence_{i // num_sentences_per_file:04d}.txt\"\n",
    "    filepath = os.path.join(\"sentences\", filename)\n",
    "    \n",
    "    with open(filepath, \"w\") as f:\n",
    "        for j in range(i, min(i + num_sentences_per_file, len(sentences))):\n",
    "            f.write(sentences[j])\n",
    "            if j != min(i + num_sentences_per_file, len(sentences)) - 1:\n",
    "                f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JCO\\AppData\\Local\\Temp\\ipykernel_1360\\2124061961.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  serie = serie.str.replace('\\\\n', ' ')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence_0000</td>\n",
       "      <td>sentence_0000. 1!!!!! ! ! !The Little Prince w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence_0001</td>\n",
       "      <td>sentence_0001. \"#$%&amp;'()*+,- /.+(*+,#+$:+/.43-#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence_0002</td>\n",
       "      <td>sentence_0002. My Drawing Number One. It looke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence_0003</td>\n",
       "      <td>sentence_0003. If one gets lost in the night, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence_0004</td>\n",
       "      <td>sentence_0004. It was a question of life or de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fname                                               text\n",
       "0  sentence_0000  sentence_0000. 1!!!!! ! ! !The Little Prince w...\n",
       "1  sentence_0001  sentence_0001. \"#$%&'()*+,- /.+(*+,#+$:+/.43-#...\n",
       "2  sentence_0002  sentence_0002. My Drawing Number One. It looke...\n",
       "3  sentence_0003  sentence_0003. If one gets lost in the night, ...\n",
       "4  sentence_0004  sentence_0004. It was a question of life or de..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list to store the text files\n",
    "texts=[]\n",
    "\n",
    "# Get all the text files in the text directory\n",
    "for file in os.listdir(\"sentences\"):\n",
    "    #print(file)\n",
    "\n",
    "    # Open the file and read the text\n",
    "    with open(\"sentences/\" + file, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
    "        texts.append((file[0:-4], text))\n",
    "\n",
    "# Create a dataframe from the list of texts\n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
    "\n",
    "# Set the text column to be the raw text with the newlines removed\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "df.to_csv('processed/scraped.csv')\n",
    "#df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
    "df.columns = ['title', 'text']\n",
    "\n",
    "# Tokenize the text and save the number of tokens to a new column\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "# Visualize the distribution of the number of tokens per row using a histogram\n",
    "#df.n_tokens.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 1800\n",
    "\n",
    "# Function to split the text into chunks of a maximum number of tokens\n",
    "def split_into_many(text, max_tokens = max_tokens):\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('. ')\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of \n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened = []\n",
    "\n",
    "# Loop through the dataframe\n",
    "for row in df.iterrows():\n",
    "\n",
    "    # If the text is None, go to the next row\n",
    "    if row[1]['text'] is None:\n",
    "        continue\n",
    "\n",
    "    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
    "    if row[1]['n_tokens'] > max_tokens:\n",
    "        shortened += split_into_many(row[1]['text'])\n",
    "    \n",
    "    # Otherwise, add the text to the list of shortened texts\n",
    "    else:\n",
    "        shortened.append( row[1]['text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(shortened, columns = ['text'])\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import distances_from_embeddings\n",
    "import csv \n",
    "import pandas as pd \n",
    "\n",
    "embeddings_path = 'processed/embeddings.csv'\n",
    "\n",
    "if os.path.exists(embeddings_path):\n",
    "    df=pd.read_csv(embeddings_path, index_col=0)\n",
    "    df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n",
    "\n",
    "else:\n",
    "    df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n",
    "    df.to_csv(embeddings_path)\n",
    "    #df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(question, df, max_len=1800, size=\"ada\"):\n",
    "    \"\"\"\n",
    "    Create a context for a question by finding the most similar context from the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the embeddings for the question\n",
    "    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
    "\n",
    "    # Get the distances from the embeddings\n",
    "    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n",
    "\n",
    "\n",
    "    returns = []\n",
    "    cur_len = 0\n",
    "\n",
    "    # Sort by distance and add the text to the context until the context is too long\n",
    "    for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
    "        \n",
    "        # Add the length of the text to the current length\n",
    "        cur_len += row['n_tokens'] + 4\n",
    "        \n",
    "        # If the context is too long, break\n",
    "        if cur_len > max_len:\n",
    "            break\n",
    "        \n",
    "        # Else add it to the text that is being returned\n",
    "        returns.append(row[\"text\"])\n",
    "\n",
    "    # Return the context\n",
    "    return \"\\n\\n###\\n\\n\".join(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    df,\n",
    "    model=\"text-davinci-003\",\n",
    "    question=\"Provide a summary of the content.\",\n",
    "    max_len=1800,\n",
    "    size=\"ada\",\n",
    "    debug=False,\n",
    "    max_tokens=150,\n",
    "    stop_sequence=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Answer a question based on the most similar context from the dataframe texts\n",
    "    \"\"\"\n",
    "    context = create_context(\n",
    "        question,\n",
    "        df,\n",
    "        max_len=max_len,\n",
    "        size=size,\n",
    "    )\n",
    "    # If debug, print the raw model response\n",
    "    if debug:\n",
    "        print(\"Context:\\n\" + context)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        # Create a completions using the question and context\n",
    "        response = openai.Completion.create(\n",
    "            prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=stop_sequence,\n",
    "            model=model,\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The book was written and illustrated by Antoine de Saint Exup√©ry.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Who wrote this book?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This book is about the Little Prince and his adventures as he travels around the universe.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is this book about?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They give it a number, such as \"Asteroid 325\".'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Waht do astronomers do when they discover a planet?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He said goodbye to the fox and took his leave.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What did the little prince do in the morning of his departure?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Little Prince meets a flower with three petals, a fox, and roses.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What animales and flowers does the Little Prince meet?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The fox teaches the little prince that \"It is only with the heart that one can see rightly; what is essential is invisible to the eye\" and that \"You become responsible, forever, for what you have tamed. You are responsible for your rose.\"'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What lessons he learns from the fox?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What lessons he learns from the flower with three petals?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The little prince crossed the desert and met with only one flower. It was a flower with three petals, a flower of no account at all. \"Good morning,\" said the little prince. \"Good morning,\" said the flower. \"Where are the men?\" the little prince asked, politely. The flower had once seen a caravan passing. \"Men?\" she echoed. \"I think there are six or seven of them in existence. I saw them, several years ago. But one never knows where to find them. The wind blows them away.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is the story with the flower with three petals?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The little prince was surprised by the flower's coquettishness and her mysterious adornment. He was also amazed that she asked for a screen to protect her from drafts. She eventually told the little prince that she loved him and asked for his forgiveness.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Anything to say about the flower with three petals?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Little Prince's planet was very small, scarcely any larger than a house. It had no people, no oceans, no mountains, no towns, no rivers, and no deserts.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Please describe the Little Prince's planet\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One lesson from this book is that true friendship cannot be bought from a shop, and it must be tamed with patience and understanding.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What can we learn from this book? please give me one lesson for me.\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
